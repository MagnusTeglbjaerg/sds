{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 17: Text as Data 2\n",
    "\n",
    "*Morning, August 23, 2018*\n",
    "\n",
    "In this Exercise Set you will practice methods within Information Extraction in python. \n",
    "You will practice the following:\n",
    "* Practice doing look ups using set operations.\n",
    "* Implement and compare different lexical based methods for sentiment analysis. \n",
    "* Furthermore you get to play with the output from a Word2Vec model and a Topic Model, both trained on 4 million reviews from the TrustPilot Review dataset, that we practiced scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 17.1: Look-ups and Dictionary Methods\n",
    "In ths exercise you will practice using curated lexicons to extract knowledge from text.\n",
    "\n",
    "First we load the dataset. Again we use the Review Data Set. Load it by running the following:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 17.1.1:**\n",
    "Define two concepts you want to measure in the reviews. And curate a list of words expressing that concept. E.g. words related to Travelling, Computers, or for the bold define words that indicate Trolling.\n",
    "* Convert the two lists into sets, and assign to variables of choice.\n",
    "\n",
    "These will be your Lexicons that you want to match up with the documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv')\n",
    "dictionaries = {'trolling':set(['lol','haha','fuck','fuuck']),\n",
    "               'thankful':set(['thank','wonderful','love'])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 17.1.2:**\n",
    "Now we design a simple preprocessing function to:\n",
    "* first tokenize the string using the nltk.word_tokenize function. \n",
    "* And secondly it converts capital letters to noncapital letters for each token, using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "# tokenize documents\n",
    "def preprocess(document):\n",
    "    tokenized = nltk.word_tokenize(document)\n",
    "    # lower all tokens\n",
    "    lowered = [i.lower() for i in tokenized]\n",
    "    return lowered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** Ex.17.1.3:**\n",
    "*Now we apply the preprocessing scheme to all of our documents assigning it to a variable: tokenized_docs.\n",
    "*Secondly we convert all of the tokenized docs into sets, by loop through the documents and applying the set() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "tokenized_docs = df.reviewBody.apply(preprocess).values\n",
    "# convert each document to a set of tokens instead of list of tokens\n",
    "doc_sets = [set(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** Ex 17.1.4:** Now we shall find the overlap between our curated lists, and each document set.\n",
    "We do this by defining a container named `overlap`. \n",
    "Then we run through all document sets:\n",
    "* And take the length of the overlap between the document set and our curated lexicons.\n",
    "* Append the length to the `overlap`  container.\n",
    "HINT: Overlaps between sets our found using the `&` sign. And length you get from the `len()` builtin function.\n",
    "* Finally assign the overlap values. a new column in the dataframe with the overlap values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.1.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "for key,lexicon in dictionaries.items(): # iterate through my curated lexicons.\n",
    "    df[key] = [len(s&lexicon)>0 for s in doc_sets] # take the overlap using a list comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise section 17.2 Lexical Based Sentiment Analysis using Dictionaries\n",
    "Here I want you to test 4 different dictionaries for sentiment analysis on the review dataset.\n",
    "\n",
    "* You will compare each document to the nltk.corpus.opinion_lexicon build into nltk.\n",
    "* You will try to use the Afinn package. `pip install afinn`\n",
    "* And finally you will compare the rulebased version of the VADER (Valence Aware Dictionary and sEntiment Reasoner - \"VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\" Hutto and Gilbert 2014) sentiment analyser, to a simple lookup version. \n",
    "\n",
    "This means comparing 4 different lexical based sentiment analysis. Two of them you will use your `set` operations for checking overlap between documents and a set. And two them will be prepackaged with builtin methods.\n",
    "\n",
    "\n",
    "> **17.2.1:** First we need to get our curated lists of words with strong signals of sentiment. The wordlists you will need for this exercise is in the nltk.corpus.opinion_lexicon. The other one is the wordlist used in the VADER method, this you will download from github using the following link: https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt\n",
    "* First assign nltk.corpus.opinion_lexicon to the variable `lexicon_1`\n",
    "* next run the following command to parse the VADER lexicon from github:\n",
    "```python\n",
    "import pandas as pd\n",
    "vader_df = pd.read_csv('https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt'\n",
    "            ,sep='\\t',header=None) # changing the separator to tab, and specifying no header\n",
    "vader_df.columns = ['token','average_score','variance','ratings'] # adding the header\n",
    "```\n",
    "* To get a list - that will be converted to a set - of posive and negative words we will extract it from the dataframe by filtering on the column average score. Take the tokens where the average score is less than 0 and assign to a variable `negative`, and do the opposite for a variable `positive`. Remember to convert them to a set. \n",
    "* Define two dictionaries for each of the curated lexicons looking like this: \n",
    "```python\n",
    "opinion_lexicon = {'positive':positive_words,'negative':negative_words}\n",
    "vader_lexicon = {'positive':vader_positive,'negative',vader_negative}\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader has 4170 negative words and 3335 positive VS. the opinion lexicon 4783 and 2006\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# get the built in lexicon\n",
    "positive_words = set(nltk.corpus.opinion_lexicon.positive())\n",
    "negative_words = set(nltk.corpus.opinion_lexicon.negative())\n",
    "# get the raw vader lexicon\n",
    "import pandas as pd\n",
    "vader_df = pd.read_csv('https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt'\n",
    "            ,sep='\\t',header=None) # changing the separator to tab, and specifying no header\n",
    "vader_df.columns = ['token','average_score','variance','ratings'] # adding the header\n",
    "# extract positive and negative words by filtering on the average score column\n",
    "vader_positive = set(vader_df[vader_df['average_score']>0].token.values)\n",
    "vader_negative = set(vader_df[vader_df['average_score']<0].token.values)\n",
    "# define two dictionaries\n",
    "vader_lexicon = {'positive':vader_positive,'negative':vader_negative}\n",
    "opinion_lexicon = {'positive':positive_words,'negative':negative_words}\n",
    "\n",
    "print('Vader has %d negative words and %d positive VS. the opinion lexicon %d and %d'%(len(vader_negative),\n",
    "                                                                                         len(vader_positive),\n",
    "                                                                                         len(negative_words),\n",
    "                                                                                         len(positive_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 17.2.3:** Scoring a document using the dicionary.\n",
    "Now we write a function that takes in a document and a dictionary containing negative and positive words. The function will tokenize the document and return a sentiment score based on the overlapping words.\n",
    "\n",
    "* First you apply the `preprocess` function you created in exercise 17.1 on the document.\n",
    "* Then you filter all words from the documents that are not in the positive word set and take the length of the resulting list. \n",
    "* You do the same with the negative word set. \n",
    "* Finally you calculate a polarity score by subtracting the negative overlap from the positive overlap and divide it by the length of the document : `pos-neg/len(doc)`    \n",
    "(Hint1: Filter like this [w for w in doc if w in pos])\n",
    "\n",
    "Wrap the above in a function called apply_sentiment_dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.2.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def apply_sentiment_dictionary(doc,dictionary):\n",
    "    doc = preprocess(doc)\n",
    "    pos = len([w for w in doc if w in dictionary['positive']])\n",
    "    neg = len([w for w in doc if w in dictionary['negative']])\n",
    "    polarity = (pos-neg)/len(doc)\n",
    "    return polarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 17.2.4:** Make two new columns in the dataframe;'opinion' ,'vader_raw', by applying the function with their respective dictionaries as input. This means you will have to give the .apply function another argument: `.apply(apply_sentiment_dictionary,args=(vader_lexicon,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer Ex. 17.2.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "df['vader_raw'] = df.reviewBody.apply(apply_sentiment_dictionary,args=(vader_lexicon,))\n",
    "df['opinion'] = df.reviewBody.apply(apply_sentiment_dictionary,args=(opinion_lexicon,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex.17.2.5:** Applying the prepackaged.\n",
    "* Figure out how to apply the Afinn method here: https://github.com/fnielsen/afinn\n",
    "* Apply the afinn score on each document and define a column 'afinn_score'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer Ex.17.2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "df['afinn_score'] = [afinn.score(doc) for doc in df.reviewBody]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex.17.2.6:** Applying the prepackaged(2).\n",
    "The MIT VADER Analyzer is run by initializing the analyzer = nltk.sentiment.SentimentIntensityAnalyzer(). And then using the builtin function of the analyzer: `.polarity_score(string)`. The function has more than one output so defining the function has more than one output, so we will only use the 'compound' variable.\n",
    "* apply the .polarity_score function to each document and extract the 'compound' value from the dictionary output of the sentiment analyzer. And define a new column called 'vader_compound' in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.2.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbv933\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import nltk.sentiment\n",
    "analyzer = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "vader_scores = [analyzer.polarity_scores(doc) for doc in df.reviewBody]\n",
    "for key in ['compound','neg','neu','pos']:\n",
    "    df['vader_%s'%key] = [i[key] for i in vader_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 17.2.7:** Comparing the performance of the Sentiment Analyzers.\n",
    "How to actually evaluate the performance of their scores does not have a definite answer, since we do not have a Human label score of each review, also they are on different scales, so what scale to use. However we might do the following:\n",
    "* Convert all ratings into a binary: 1 if above 3 and 0 if below. \n",
    "* Do the same with the Scores from the sentiment analyzers.\n",
    "* And calculate an accuracy score. \n",
    "\n",
    "or \n",
    "> \n",
    "* we could do a simple correlation between the score and the rating. And compare which has the best fit.\n",
    "    * use np.correcoef()\n",
    "* or even train a classifier to predict the Rating using the output from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analyzer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>correlation</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vader_raw</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.329029</td>\n",
       "      <td>0.896989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vader_compound</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.546860</td>\n",
       "      <td>0.900119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afinn_score</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.384245</td>\n",
       "      <td>0.900045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>opinion</td>\n",
       "      <td>0.7943</td>\n",
       "      <td>0.343067</td>\n",
       "      <td>0.906115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Analyzer  accuracy  correlation  Precision\n",
       "0       vader_raw    0.7776     0.329029   0.896989\n",
       "1  vader_compound    0.8318     0.546860   0.900119\n",
       "2     afinn_score    0.8032     0.384245   0.900045\n",
       "3         opinion    0.7943     0.343067   0.906115"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Answer 17.2.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analyzer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>correlation</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vader_raw</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.329029</td>\n",
       "      <td>0.896989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vader_compound</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.546860</td>\n",
       "      <td>0.900119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afinn_score</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.384245</td>\n",
       "      <td>0.900045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>opinion</td>\n",
       "      <td>0.7943</td>\n",
       "      <td>0.343067</td>\n",
       "      <td>0.906115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Analyzer  accuracy  correlation  Precision\n",
       "0       vader_raw    0.7776     0.329029   0.896989\n",
       "1  vader_compound    0.8318     0.546860   0.900119\n",
       "2     afinn_score    0.8032     0.384245   0.900045\n",
       "3         opinion    0.7943     0.343067   0.906115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "y = [1 if i>3 else 0 for i in df.reviewRating_ratingValue] # Convert rating to binary\n",
    "header = ['Analyzer','accuracy','correlation','Precision']\n",
    "performance = []\n",
    "for num,column in enumerate(['vader_raw','vader_compound','afinn_score','opinion']):\n",
    "    subdf = df[df[column]!=0] # filter on the values where the sentiment score is defined.\n",
    "    y = np.array([1 if i>3 else 0 for i in subdf.reviewRating_ratingValue])\n",
    "    x = np.array([1 if i>0 else 0 for i in df[column] if not i==0]) # convert the sentiment score to a binary\n",
    "    accuracy = sum(x==y)/len(df)\n",
    "    # calculate the correlation\n",
    "    correlation = np.corrcoef(df[column],df['reviewRating_ratingValue'])[0][1]\n",
    "    row = [column,accuracy,correlation,sklearn.metrics.accuracy_score(y,x)]\n",
    "    performance.append(row)\n",
    "pd.DataFrame(performance,columns=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 17.3: Playing around with Outputs from Unsupervised Models\n",
    "Here I want you to get acquinted with the capabilities and the syntax in the python implementation of the two famous unsupervised methods for text data: Topic Modelling and Word2Vec. \n",
    "\n",
    "You need to install the pyldaviz package: `conda install -c conda-forge pyldavis`\n",
    "\n",
    "\n",
    "Download the Word2Vec model here: https://www.dropbox.com/sh/lwpoyipspunzojl/AABSoO8j7EUjPLixSBkOe7Uda?dl=0\n",
    "\n",
    "Download the TopicModel here: https://www.dropbox.com/sh/fmmxcyvnti0c1y7/AAAOgHmnD2mbbHEQiwtJsjW-a?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load('word_embeddings_review/w2vec_review')\n",
    "from gensim.models import LdaMulticore\n",
    "lda = LdaMulticore.load('topicmodel_review/lda50_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** Ex.17.3.1: ** The Word2Vec model object. \n",
    "Here we will use the `model.wv.most_similar()` method to seach the vector space. This can be used when developing lexicons.\n",
    "We will see how the model has embedded the negative and positive words from our lexicons.\n",
    "* First we define a union between the Vader lexicon and the Opinion lexicon. The union of two sets can be done using the `|` operator.\n",
    "* Then we filter which of these words are actually in the vocabulary of the model. The vocabulary of the model can be found under the model.vocab property, and then you use an `if in` statement to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670 2530\n"
     ]
    }
   ],
   "source": [
    "all_negative = vader_negative|negative_words\n",
    "all_positive = vader_positive|positive_words\n",
    "\n",
    "in_w2vec_positive = [w for w in all_positive if w in model.wv]\n",
    "in_w2vec_negative = [w for w in all_negative if w in model.wv]\n",
    "print(len(in_w2vec_negative),len(in_w2vec_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 17.3.2:**\n",
    "Now we pick a random sample from the negative and apply the .most_similar command. \n",
    "* We use the `random` module and the random.choice method to get a word.\n",
    "* And then we `print(word,model.wv.most_similar(word))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance [('submission', 0.5592600107192993), ('denial', 0.5328542590141296), ('approval', 0.5284862518310547), ('completion', 0.5146688222885132), ('finalization', 0.4938328266143799), ('eligibility', 0.48998504877090454), ('issuance', 0.48656946420669556), ('underwriting', 0.4714069366455078), ('submitting', 0.4659322202205658), ('acknowledgment', 0.46115410327911377)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbv933\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "import random\n",
    "w = random.choice(in_w2vec_positive)\n",
    "print(w,model.wv.most_similar(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.17.3.3:** Now we do some of the famous linear algebra (King - Man + Women = Queen)\n",
    "But instead we say: What is good- :) + :( = ? \n",
    "We do this by applying the same function:\n",
    "`.most_similar(positive=['good',':('],negative=[':)'])` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 17.3.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbv933\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('poor', 0.6456157565116882),\n",
       " ('terrible', 0.5774315595626831),\n",
       " ('decent', 0.5770268440246582),\n",
       " ('bad', 0.5701460838317871),\n",
       " ('subpar', 0.5187545418739319),\n",
       " ('horrible', 0.5081883668899536),\n",
       " ('disappointing', 0.5017893314361572),\n",
       " ('awful', 0.4973694682121277),\n",
       " ('lousy', 0.4924020767211914),\n",
       " ('substandard', 0.48289310932159424)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "model.wv.most_similar(positive=['good',':('],negative=[':)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Exercise 17.3.5:** Interactive Plotting of Word Embedding\n",
    "** Inspecting clusters of Words ** \n",
    "Run PCA on a subsample of the wordvectors found by applying this command.\n",
    "* Inspect what the different dimensions seem to represent by hovering over the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.offline as py # import plotly in offline mode\n",
    "py.init_notebook_mode(connected=True) # initialize the offline mode, with access to the internet or not.\n",
    "import plotly.tools as tls \n",
    "tls.embed('https://plot.ly/~cufflinks/8') # embed cufflinks.\n",
    "# import cufflinks and make it offline\n",
    "import cufflinks as cf\n",
    "cf.go_offline() # initialize cufflinks in offline mode\n",
    "import random\n",
    "negative_sample = random.sample(list(in_w2vec_negative),200)\n",
    "positive_sample = random.sample(list(in_w2vec_positive),200)\n",
    "neutral = random.sample(list(model.wv.vocab),600)\n",
    "words = negative_sample+positive_sample+neutral\n",
    "valence = (['Positive']*200)+(['Negative']*200)+(['Neutral']*600)\n",
    "X = [model.wv[w] for w in words]\n",
    "#from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "#embedding = TSNE(n_components=2).fit_transform(X)\n",
    "embedding = PCA(n_components=2).fit_transform(X)\n",
    "embedding_df = pd.DataFrame(embedding,columns=['x','y'])\n",
    "embedding_df['word'] = words\n",
    "embedding_df['valence'] = valence\n",
    "embedding_df.iplot(x='x',y='y',categories='valence',text='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex.17.3.6:** Inspecting TOPIC MODELS using pyldaviz\n",
    "Lets look at the ldamodel object. We shall use the pyldaviz package to \"discover\" what the topic model have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "import pickle\n",
    "sample_corpus = pickle.load(open('topicmodel_review/lda_sample_corpus.pkl','rb'))\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda,corpus=sample_corpus,dictionary=lda.id2word) # this takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
